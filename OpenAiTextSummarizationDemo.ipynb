{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-XU3_aRitCbl",
        "outputId": "199f3861-7ab6-467a-8944-cd59a653b21e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting openai\n",
            "  Downloading openai-0.25.0.tar.gz (44 kB)\n",
            "\u001b[K     |████████████████████████████████| 44 kB 1.8 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from openai) (1.21.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from openai) (4.64.1)\n",
            "Requirement already satisfied: pandas>=1.2.3 in /usr/local/lib/python3.8/dist-packages (from openai) (1.3.5)\n",
            "Collecting pandas-stubs>=1.1.0.11\n",
            "  Downloading pandas_stubs-1.5.2.221124-py3-none-any.whl (146 kB)\n",
            "\u001b[K     |████████████████████████████████| 146 kB 9.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from openai) (4.1.1)\n",
            "Requirement already satisfied: openpyxl>=3.0.7 in /usr/local/lib/python3.8/dist-packages (from openai) (3.0.10)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.8/dist-packages (from openai) (2.23.0)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.8/dist-packages (from openpyxl>=3.0.7->openai) (1.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.2.3->openai) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.2.3->openai) (2022.6)\n",
            "Collecting types-pytz>=2022.1.1\n",
            "  Downloading types_pytz-2022.6.0.1-py3-none-any.whl (4.7 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas>=1.2.3->openai) (1.15.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20->openai) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20->openai) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20->openai) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20->openai) (2.10)\n",
            "Building wheels for collected packages: openai\n",
            "  Building wheel for openai (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai: filename=openai-0.25.0-py3-none-any.whl size=55880 sha256=22c5ee480d4497bcfdf1849de67be1f58d9124b56a8873997d213756768c9e8e\n",
            "  Stored in directory: /root/.cache/pip/wheels/4b/92/33/6f57c7aae0b16875267999a50570e81f15eecec577ebe05a2e\n",
            "Successfully built openai\n",
            "Installing collected packages: types-pytz, pandas-stubs, openai\n",
            "Successfully installed openai-0.25.0 pandas-stubs-1.5.2.221124 types-pytz-2022.6.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pymupdf"
      ],
      "metadata": {
        "id": "ZSfTikpK0Bu5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "babf4b5d-6ac4-4500-f5c9-f4bfe5372ba5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pymupdf\n",
            "  Downloading PyMuPDF-1.21.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 14.0 MB 5.2 MB/s \n",
            "\u001b[?25hInstalling collected packages: pymupdf\n",
            "Successfully installed pymupdf-1.21.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "OPENAI_API_KEY = ''\n",
        "with open('/content/OpenAI.json', 'r') as file_to_read:\n",
        "    json_data = json.load(file_to_read)\n",
        "    OPENAI_API_KEY = json_data[\"OPENAI_API_KEY\"]"
      ],
      "metadata": {
        "id": "RfmpkMqT1daE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "\n",
        "openai.api_key =  OPENAI_API_KEY"
      ],
      "metadata": {
        "id": "XajkO-wEtnXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz"
      ],
      "metadata": {
        "id": "srz45bfF0DJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = fitz.open('/content/Attention.pdf')"
      ],
      "metadata": {
        "id": "vfWpy7_QVC8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary_list =[]\n",
        "for page in doc:\n",
        "  text = page.get_text(\"text\")\n",
        "  #print(text)\n",
        "  prompt= text + \"\\n Tl;dr:\"\n",
        "  response = openai.Completion.create(\n",
        "  model=\"text-davinci-003\",\n",
        "  prompt=prompt,\n",
        "  temperature=0.7,\n",
        "  max_tokens=120,\n",
        "  top_p=0.9,\n",
        "  frequency_penalty=0.0,\n",
        "  presence_penalty=1\n",
        "  )\n",
        "  summary_list.append(response[\"choices\"][0][\"text\"])\n"
      ],
      "metadata": {
        "id": "Ni19lQsoVJKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary_text=' '.join(summary_list)\n",
        "print(summary_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bUjynEGNXcbT",
        "outputId": "5f539c10-a2c8-46ed-e909-bbda65cc1a8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " This paper proposes a new network architecture, the Transformer, based solely on attention mechanisms, which improves machine translation quality while being more parallelizable and requiring less time to train. Experiments on two machine translation tasks show that the model achieves superior results, improving over the existing best results by over 2 BLEU points.  This paper proposes the Transformer, a model architecture that eschews recurrence and instead relies entirely on an attention mechanism to draw global dependencies between input and output. This allows for more parallelization, leading to improved performance in translation quality. Additionally, the paper discusses self-attention, end-to-end memory networks, and the overall Transformer architecture. \n",
            "\n",
            "The Transformer model consists of an encoder and decoder stack, each composed of 6 layers. Each layer contains two sub-layers: a multi-head self-attention mechanism and a position-wise fully connected feed-forward network. In the decoder stack, a third sub-layer performs multi-head attention over the output of the encoder stack. Attention is a mapping between a query and key-value pairs to an output vector, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key. \n",
            "Scaled Dot-Product Attention is a type of attention mechanism that uses dot products to compute the weights on the values. It is faster and more space-efficient than additive attention, as it can be implemented using highly optimized matrix multiplication code. Multi-Head Attention consists of several attention layers running in parallel, which are projected onto different dimensions before being concatenated and projected again. Scaling the dot products by 1/√dk helps counteract the effect of large magnitudes for large values of dk.  Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. It is implemented using h=8 parallel attention layers, with each head having a reduced dimension of dk=dv=dmodel/h=64. This is used in the encoder-decoder attention layer, the encoder self-attention layer and the decoder self-attention layer. In addition, the model contains position-wise feed-forward networks, embeddings and softmax, and positional encoding to inject order into the sequence. \n",
            "Self-attention layers offer a number of advantages over recurrent and convolutional layers when mapping one variable-length sequence to another, including lower computational complexity per layer, the ability to parallelize operations, and shorter paths between long-range dependencies. They are faster than recurrent layers when the sequence length is smaller than the representation dimensionality, which is often the case with sentence representations used in machine translations. Self-attention can also be restricted to considering only a neighborhood of size r for improved performance on very long sequences. \n",
            "This paper describes a model for machine translation that uses self-attention instead of recurrent layers. The model is trained on the WMT 2014 English-German and English-French datasets, using an Adam optimizer with residual dropout and other regularization techniques. Results show that the model achieves state-of-the-art results in both language pairs.  The Transformer model achieved better BLEU scores than previous state-of-the-art models on the English-to-German and English-to-French newstest2014 tests at a fraction of the training cost. This model was trained using label smoothing, beam search, and checkpoint averaging. Additionally, we evaluated the importance of different components of the Transformer by varying our base model in different ways and measuring the change in performance on English-to-German translation on the development set.  In Table 3, we observe that reducing the attention key size dk hurts model quality, suggesting that a more sophisticated compatibility function than dot product may be beneficial. We also observe that bigger models are better and dropout is very helpful in avoiding over-fitting. In row (E), we replace our sinusoidal positional encoding with learned positional embeddings and observe nearly identical results to the base model. For English constituency parsing, we find that the Transformer generalizes well, achieving comparable results to previous state-of-the-art approaches.  In this paper, we present the Transformer, a sequence transduction model based entirely on attention. We show that it outperforms recurrent and convolutional architectures for tasks such as translation. We also present results showing that our model performs surprisingly well in a semi-supervised setting with only 40K training sentences.  Recurrent Neural Networks (RNNs) have been used in many natural language processing tasks such as sequence modeling, grammar learning, and machine translation. They have been shown to be effective at capturing long-term dependencies and can be improved through gating mechanisms and self-attention. Recent advances such as Neural GPUs and Structured Attention Networks allow for faster training of RNNs and better performance on tasks.  This paper presents a review of current deep learning architectures for Natural Language Processing, including Recurrent Neural Networks (RNNs), Convolutional Neural Networks (CNNs), Attention Models, Reinforced Models, Output Embeddings, Subword Units, Mixture-of-Experts Layer, Dropout, End-to-End Memory Networks, Sequence to Sequence Learning, Inception Architecture, Grammar as a Foreign Language, Google's Neural Machine Translation System and Shift-Reduce Constituent Parsing.  An example of the attention mechanism in layer 5 of 6 of the encoder self-attention shows that many of the attention heads attend to a distant dependency of the verb ‘making’, completing the phrase ‘making...more difficult’. Different colors represent different heads.  Attention heads 5 and 6 in layer 5 of 6 appear to be involved in anaphora resolution, as evidenced by the sharp attentions they give to the word 'its'.  Different attention heads in the encoder self-attention layer of a transformer model have learned to perform different tasks related to the structure of the sentence, as evidenced by two examples from two different heads at layer 5.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt= summary_text + \"\\n Tl;dr:\"\n",
        "response = openai.Completion.create(\n",
        "model=\"text-davinci-003\",\n",
        "prompt=prompt,\n",
        "temperature=0.7,\n",
        "max_tokens=400,\n",
        "top_p=0.9,\n",
        "frequency_penalty=0.0,\n",
        "presence_penalty=1\n",
        ")\n",
        "print(response[\"choices\"][0][\"text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrN3AgYzZHzd",
        "outputId": "69cb4077-e24b-4e44-c82d-054dd24ee4d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " This paper presents a new architecture, the Transformer, which relies on attention mechanisms instead of recurrence and is more parallelizable. Experiments on two machine translation tasks show that the model achieves superior results, improving over the existing best results by 2 BLEU points. It contains an encoder and decoder stack with multi-head self-attention mechanisms and position-wise fully connected feed-forward networks. Attention heads in the encoder self-attention layer have learned to perform different tasks related to the structure of the sentence.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Np0zcc0IajDP",
        "outputId": "080ce32a-c04b-40f9-fd65-2a2b806520b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"choices\": [\n",
            "    {\n",
            "      \"finish_reason\": \"stop\",\n",
            "      \"index\": 0,\n",
            "      \"logprobs\": null,\n",
            "      \"text\": \" This paper presents a new architecture, the Transformer, which relies on attention mechanisms instead of recurrence and is more parallelizable. Experiments on two machine translation tasks show that the model achieves superior results, improving over the existing best results by 2 BLEU points. It contains an encoder and decoder stack with multi-head self-attention mechanisms and position-wise fully connected feed-forward networks. Attention heads in the encoder self-attention layer have learned to perform different tasks related to the structure of the sentence.\"\n",
            "    }\n",
            "  ],\n",
            "  \"created\": 1670219783,\n",
            "  \"id\": \"cmpl-6Jykhkghj0VSQG0tokyjPRpg6w9OA\",\n",
            "  \"model\": \"text-davinci-003\",\n",
            "  \"object\": \"text_completion\",\n",
            "  \"usage\": {\n",
            "    \"completion_tokens\": 108,\n",
            "    \"prompt_tokens\": 1262,\n",
            "    \"total_tokens\": 1370\n",
            "  }\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary_list =[]\n",
        "for page in doc:\n",
        "  text = page.get_text(\"text\")\n",
        "  #print(text)\n",
        "  prompt= \"Summarize this for a second-grade student: \" +text\n",
        "  response = openai.Completion.create(\n",
        "  model=\"text-davinci-003\",\n",
        "  prompt=prompt,\n",
        "  temperature=0.7,\n",
        "  max_tokens=120,\n",
        "  top_p=0.9,\n",
        "  frequency_penalty=0.0,\n",
        "  presence_penalty=1\n",
        "  )\n",
        "  summary_list.append(response[\"choices\"][0][\"text\"])\n"
      ],
      "metadata": {
        "id": "r4WmT6W5ePIk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary_text= ' '.join(summary_list)\n",
        "print(summary_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUYSaUW6f8-H",
        "outputId": "e5f381a3-b882-4101-e96a-e58f2ce35419"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Scientists have created a new type of network architecture called the Transformer that uses attention mechanisms instead of recurrence and convolutions to help machines understand and translate language. Experiments have shown that this model is faster and more accurate than other models that have been used before. It has helped machines to do things like translate English into German or French and understand how words fit together in a sentence. A Transformer is a type of model used for language modeling and machine translation. It uses attention mechanisms to draw global dependencies between input and output, which allows for more parallelization and faster training times. For a second-grade student:\n",
            "The Transformer is a model that helps machines understand and generate language. It has two parts - an encoder which takes in the language and a decoder which produces the output language. It also uses something called attention, which is like a way for the machine to remember important words from the input language and use them in the output language. \n",
            "Multi-Head Attention is a type of attention used in computer programming. It consists of several layers that run in parallel, each layer taking the queries, keys, and values and projecting them into different dimensions. The output is then combined and projected to create the final values. Scaled Dot-Product Attention is a type of Multi-Head Attention which scales the dot products by 1 divided by the square root of the dimension of the keys. This helps make sure that the softmax function works correctly. tokens in the sequence. To this end, we add “positional encodings” to the input embeddings at the\n",
            "bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\n",
            "as the embeddings, so that the two can be summed. There are many choices of positional encodings,\n",
            "learned and fixed [18].\n",
            "\n",
            "Multi-head attention is a way for a computer model to look at different parts of a sentence or other text at the same time. It uses 8 different \" Self-attention is a way of connecting each part of a sequence to every other part. It can be used to help machines understand longer sequences, like sentences. Self-attention layers take less time than recurrent layers and have shorter paths between long-range dependencies. A second-grade student might understand this as: Scientists are trying to create a better way for computers to understand language. They are using something called \"self-attention\" which helps computers understand the words and their meanings in sentences. They use layers of different parts in their models to help with this. This process is complex and takes time, but it may help computers better understand language. A second-grade student can understand that the Transformer is a machine learning model that can achieve better scores than other models on English-to-German and English-to-French tests, while using less training cost. It achieved the highest BLEU score of 28.4 on English-to-German translation and 41.0 on English-to-French translation. \n",
            "The Transformer is a type of architecture that can help with translating and understanding language. It can be changed in different ways to make it work better. When the size of the model is bigger and dropout is used, it works better. The Transformer also works well when doing English constituency parsing which is a type of language understanding. A computer model called the Transformer was trained on 40,000 sentences to learn how to translate between languages. It was able to outperform other models and is now used to help computers understand human language better. Four people (Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio) studied how computers can understand things that are written or said. They used something called a recurrent neural network to help them. Four people, Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit, wrote a paper about a special type of computer model. Romain Paulus, Caiming Xiong, and Richard Socher wrote a paper about a deep learning model that helps computers summarize information. Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein wrote a paper about teaching computers to understand tree diagrams. Oﬁr Press and Lior Wolf wrote a paper about making computers better at language. Rico Senn \n",
            "Many governments in America have passed new laws since 2009 which make it harder to register or vote. We need to make sure that the law is applied fairly and justly, which is something we are missing. The law can never be perfect, but it should be applied in a just way - this is something we are missing.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt= \"Summarize this for a second-grade student: \"+summary_text\n",
        "response = openai.Completion.create(\n",
        "model=\"text-davinci-003\",\n",
        "prompt=prompt,\n",
        "temperature=0.7,\n",
        "max_tokens=400,\n",
        "top_p=0.9,\n",
        "frequency_penalty=0.0,\n",
        "presence_penalty=1\n",
        ")\n",
        "print(response[\"choices\"][0][\"text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nScXhI01gDiK",
        "outputId": "d64e1c82-ecea-4bb8-c42e-72e7f9f45afe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Scientists have been looking for ways to make computers understand language better. They have created a model called the Transformer that uses attention mechanisms and is able to translate quickly and accurately. This model has been tested on two machine translation tasks and was found to be more accurate and faster than existing methods. It also works well for English constituency parsing.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T3hZO60FgU_3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}